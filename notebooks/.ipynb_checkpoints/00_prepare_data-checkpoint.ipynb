{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237b91d64ccafcdd",
   "metadata": {},
   "source": [
    "# Exploration of the PlantVillage Dataset (Introduction/Initial Steps)\n",
    "This notebook demonstrates the initial steps to explore the PlantVillage dataset.\n",
    "We will extract the dataset, check its structure, and prepare it for further analysis.\n",
    "Link to the dataset: [PlantVillage Dataset](https://www.kaggle.com/datasets/emmarex/plantdisease)\n",
    "\n",
    "### Requirements:\n",
    "Install all dependencies from the `requirements.txt` file\n",
    "and verify that the environment is correctly set up."
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "%pip install -r ../requirements.txt",
   "id": "60d2e882e062ba86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip list",
   "id": "6162fec5174a4697"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Import dependencies and set up paths\n",
    "We import all necessary libraries and configuration paths from `src/config.py`.\n",
    "The configuration file keeps directory paths centralized to avoid hardcoding them across the project."
   ],
   "id": "6169b33a3f2c0e81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T20:18:05.629599Z",
     "start_time": "2025-11-07T20:18:05.618209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from src.config import DATA_RAW_DIR, DATA_PROCESSED_DIR\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Libraries and configuration loaded successfully.\")\n",
    "print(f\"Raw data folder: {DATA_RAW_DIR}\")\n",
    "print(f\"Processed data folder: {DATA_PROCESSED_DIR}\")"
   ],
   "id": "a55c6498ab857fd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and configuration loaded successfully.\n",
      "Raw data folder: C:\\Users\\Alexandre\\PycharmProjects\\PlantVillageMachineLearning\\data\\raw\n",
      "Processed data folder: C:\\Users\\Alexandre\\PycharmProjects\\PlantVillageMachineLearning\\data\\processed\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Extract the PlantVillage ZIP archive\n",
    "We first check if the dataset has already been extracted.\n",
    "If not, we extract it into `data/processed/PlantVillage`."
   ],
   "id": "496d726379c69875"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T20:18:48.757097Z",
     "start_time": "2025-11-07T20:18:05.682055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_zip = Path(DATA_RAW_DIR) / \"PlantVillageDataset.zip\"\n",
    "extract_dir = Path(DATA_PROCESSED_DIR) / \"PlantVillage\"\n",
    "\n",
    "if not extract_dir.exists():\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not raw_zip.exists():\n",
    "        raise FileNotFoundError(f\"Zip file not found at: {raw_zip.resolve()}\")\n",
    "    with zipfile.ZipFile(raw_zip, \"r\") as z:\n",
    "        z.extractall(extract_dir)\n",
    "    print(f\"Dataset extracted to: {extract_dir.resolve()}\")\n",
    "else:\n",
    "    print(f\"Dataset already extracted at: {extract_dir.resolve()}\")"
   ],
   "id": "c0e0b6a1710cfce5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to: C:\\Users\\Alexandre\\PycharmProjects\\PlantVillageMachineLearning\\data\\processed\\PlantVillage\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Check dataset structure\n",
    "Some datasets include an extra inner folder (e.g. `PlantVillage/PlantVillage/`).\n",
    "We automatically detect and handle that case."
   ],
   "id": "1b876dd8d24223cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T20:18:57.187979Z",
     "start_time": "2025-11-07T20:18:57.182813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "entries = list(extract_dir.iterdir())\n",
    "subdirs = [p for p in entries if p.is_dir()]\n",
    "image_exts = ('.jpg', '.jpeg', '.png')\n",
    "has_images = any(p.is_file() and p.suffix.lower() in image_exts for p in entries)\n",
    "\n",
    "if len(subdirs) == 1 and not has_images:\n",
    "    dataset_root = subdirs[0]\n",
    "    print(f\"Detected single inner folder: {dataset_root.name}\")\n",
    "else:\n",
    "    dataset_root = extract_dir\n",
    "\n",
    "print(f\"Dataset root set to: {dataset_root}\")"
   ],
   "id": "4936d964b8b9a43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected single inner folder: PlantVillage\n",
      "Dataset root set to: C:\\Users\\Alexandre\\PycharmProjects\\PlantVillageMachineLearning\\data\\processed\\PlantVillage\\PlantVillage\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Prepare train and validation folders\n",
    "We create new `train` and `val` directories under `data/processed/`,\n",
    "cleaning any existing ones to ensure a fresh split."
   ],
   "id": "b3cf7e4ba5d57860"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T20:18:57.303948Z",
     "start_time": "2025-11-07T20:18:57.297507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir = Path(DATA_PROCESSED_DIR) / \"train\"\n",
    "val_dir = Path(DATA_PROCESSED_DIR) / \"val\"\n",
    "\n",
    "for d in (train_dir, val_dir):\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Train and validation directories are ready.\")"
   ],
   "id": "d2d5ec8796ff0e7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validation directories are ready.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Split the dataset\n",
    "We shuffle all images in each class and split them into:\n",
    "* 80% training data\n",
    "* 20% validation data\n",
    "\n",
    "Then, we copy (or move) the images into their respective class folders."
   ],
   "id": "e93cbd47eccab135"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T20:19:16.748762Z",
     "start_time": "2025-11-07T20:18:57.360008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_ratio = 0.8\n",
    "classes = [p for p in dataset_root.iterdir() if p.is_dir()]\n",
    "print(f\"Found {len(classes)} classes:\")\n",
    "\n",
    "image_exts = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "for class_path in classes:\n",
    "    print(\" -\", class_path.name)\n",
    "\n",
    "    images = [p for p in class_path.iterdir() if p.is_file() and p.suffix.lower() in image_exts]\n",
    "    if not images:\n",
    "        print(f\"No images found in {class_path.name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    random.shuffle(images)\n",
    "    split_idx = int(len(images) * split_ratio)\n",
    "    train_images = images[:split_idx]\n",
    "    val_images = images[split_idx:]\n",
    "\n",
    "    (train_dir / class_path.name).mkdir(parents=True, exist_ok=True)\n",
    "    (val_dir / class_path.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for img in train_images:\n",
    "        shutil.copy(img, train_dir / class_path.name / img.name)\n",
    "    for img in val_images:\n",
    "        shutil.copy(img, val_dir / class_path.name / img.name)\n",
    "\n",
    "    print(f\"{class_path.name}: {len(train_images)} train, {len(val_images)} val\")\n",
    "\n",
    "print(\"Dataset split completed.\")"
   ],
   "id": "b6a0430f82e6231a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 classes:\n",
      " - Pepper__bell___Bacterial_spot\n",
      "Pepper__bell___Bacterial_spot: 797 train, 200 val\n",
      " - Pepper__bell___healthy\n",
      "Pepper__bell___healthy: 1182 train, 296 val\n",
      " - PlantVillage\n",
      "No images found in PlantVillage, skipping.\n",
      " - Potato___Early_blight\n",
      "Potato___Early_blight: 800 train, 200 val\n",
      " - Potato___healthy\n",
      "Potato___healthy: 121 train, 31 val\n",
      " - Potato___Late_blight\n",
      "Potato___Late_blight: 800 train, 200 val\n",
      " - Tomato_Bacterial_spot\n",
      "Tomato_Bacterial_spot: 1701 train, 426 val\n",
      " - Tomato_Early_blight\n",
      "Tomato_Early_blight: 800 train, 200 val\n",
      " - Tomato_healthy\n",
      "Tomato_healthy: 1272 train, 319 val\n",
      " - Tomato_Late_blight\n",
      "Tomato_Late_blight: 1527 train, 382 val\n",
      " - Tomato_Leaf_Mold\n",
      "Tomato_Leaf_Mold: 761 train, 191 val\n",
      " - Tomato_Septoria_leaf_spot\n",
      "Tomato_Septoria_leaf_spot: 1416 train, 355 val\n",
      " - Tomato_Spider_mites_Two_spotted_spider_mite\n",
      "Tomato_Spider_mites_Two_spotted_spider_mite: 1340 train, 336 val\n",
      " - Tomato__Target_Spot\n",
      "Tomato__Target_Spot: 1123 train, 281 val\n",
      " - Tomato__Tomato_mosaic_virus\n",
      "Tomato__Tomato_mosaic_virus: 298 train, 75 val\n",
      " - Tomato__Tomato_YellowLeaf__Curl_Virus\n",
      "Tomato__Tomato_YellowLeaf__Curl_Virus: 2566 train, 642 val\n",
      "Dataset split completed.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6: Verify the split\n",
    "We check that both the training and validation sets contain the expected number of images."
   ],
   "id": "c693b5a05ac01f32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T20:19:17.370435Z",
     "start_time": "2025-11-07T20:19:16.786414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_images(folder):\n",
    "    exts = ('.jpg', '.jpeg', '.png')\n",
    "    return sum(1 for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts)\n",
    "\n",
    "train_count = count_images(train_dir)\n",
    "val_count = count_images(val_dir)\n",
    "\n",
    "print(f\"Total training images: {train_count}\")\n",
    "print(f\"Total validation images: {val_count}\")"
   ],
   "id": "2deade7e0364e6b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 16504\n",
      "Total validation images: 4134\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "We now have a clean and well-structured dataset ready for model training.\n",
    "\n",
    "* The dataset was extracted and split successfully.\n",
    "* Train and validation directories follow the same class hierarchy.\n",
    "* Randomization ensures a fair split for evaluation.\n",
    "\n",
    "Next step: we can load the datasets in TensorFlow or PyTorch using `image_dataset_from_directory()` and start training the CNN model."
   ],
   "id": "3df89775f82632bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237b91d64ccafcdd",
   "metadata": {},
   "source": [
    "# Preparation of the PlantVillage Dataset (Introduction/Initial Steps)\n",
    "This notebook demonstrates the initial steps to prepare the PlantVillage dataset\n",
    "for training a Convolutional Neural Network (CNN) model to classify plant diseases.\n",
    "We will extract the dataset, check its structure, and prepare it for further analysis.\n",
    "Link to the dataset: [PlantVillage Dataset](https://www.kaggle.com/datasets/emmarex/plantdisease)\n",
    "\n",
    "### Requirements:\n",
    "Install all dependencies from the `requirements.txt` file\n",
    "and verify that the environment is correctly set up."
   ]
  },
  {
   "cell_type": "code",
   "id": "60d2e882e062ba86",
   "metadata": {},
   "source": [
    "%pip install -r ../requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6162fec5174a4697",
   "metadata": {},
   "source": [
    "%pip list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6169b33a3f2c0e81",
   "metadata": {},
   "source": [
    "## Step 1: Import dependencies and set up paths\n",
    "We import all necessary libraries and configuration paths from `src/config.py`.\n",
    "The configuration file keeps directory paths centralized to avoid hardcoding them across the project."
   ]
  },
  {
   "cell_type": "code",
   "id": "a55c6498ab857fd1",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from src.config import DATA_RAW_DIR, DATA_PROCESSED_DIR, MODELS_DIR\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Libraries and configuration loaded successfully.\")\n",
    "print(f\"Raw data folder: {DATA_RAW_DIR}\")\n",
    "print(f\"Processed data folder: {DATA_PROCESSED_DIR}\")\n",
    "print(f\"Models folder: {MODELS_DIR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "496d726379c69875",
   "metadata": {},
   "source": [
    "## Step 2: Extract the PlantVillage ZIP archive\n",
    "We first check if the dataset has already been extracted.\n",
    "If not, we extract it into `data/processed/PlantVillage`."
   ]
  },
  {
   "cell_type": "code",
   "id": "c0e0b6a1710cfce5",
   "metadata": {},
   "source": [
    "# Ensure base folders exist\n",
    "for folder in [DATA_RAW_DIR, DATA_PROCESSED_DIR, MODELS_DIR]:\n",
    "    if not folder.exists():\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Created missing directory: {folder.resolve()}\")\n",
    "\n",
    "zip_files = list(Path(DATA_RAW_DIR).glob(\"*.zip\"))\n",
    "if not zip_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No ZIP file found in: {DATA_RAW_DIR.resolve()}\\n\"\n",
    "        \"Please download the PlantVillage dataset from Kaggle:\\n\"\n",
    "        \"https://www.kaggle.com/datasets/emmarex/plantdisease\\n\"\n",
    "        \"Then place the ZIP file inside the /data/raw directory.\"\n",
    "    )\n",
    "else:\n",
    "    raw_zip = zip_files[0]\n",
    "    print(f\"Found ZIP file: {raw_zip.name}\")\n",
    "\n",
    "extract_dir = Path(DATA_PROCESSED_DIR) / \"PlantVillage\"\n",
    "\n",
    "if not extract_dir.exists():\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Extracting PlantVillage dataset... please wait.\")\n",
    "    with zipfile.ZipFile(raw_zip, \"r\") as z:\n",
    "        z.extractall(extract_dir)\n",
    "    print(f\"Dataset extracted successfully to: {extract_dir.resolve()}\")\n",
    "else:\n",
    "    print(f\"Dataset already extracted at: {extract_dir.resolve()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b876dd8d24223cc",
   "metadata": {},
   "source": [
    "## Step 3: Check dataset structure\n",
    "Some datasets include an extra inner folder (e.g. `PlantVillage/PlantVillage/`).\n",
    "We automatically detect and handle that case."
   ]
  },
  {
   "cell_type": "code",
   "id": "4936d964b8b9a43",
   "metadata": {},
   "source": [
    "entries = list(extract_dir.iterdir())\n",
    "subdirs = [p for p in entries if p.is_dir()]\n",
    "image_exts = ('.jpg', '.jpeg', '.png')\n",
    "has_images = any(p.is_file() and p.suffix.lower() in image_exts for p in entries)\n",
    "\n",
    "if len(subdirs) == 1 and not has_images:\n",
    "    inner = subdirs[0]\n",
    "    print(f\"Detected extra inner folder: {inner.name}. Flattening...\")\n",
    "    for item in inner.iterdir():\n",
    "        shutil.move(str(item), extract_dir / item.name)\n",
    "    inner.rmdir()\n",
    "else:\n",
    "    print(\"No flatten needed.\")\n",
    "\n",
    "dataset_root = extract_dir\n",
    "print(f\"Dataset root set to: {dataset_root}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3cf7e4ba5d57860",
   "metadata": {},
   "source": [
    "## Step 4: Prepare train and validation folders\n",
    "We create new `train` and `val` directories under `data/processed/`,\n",
    "cleaning any existing ones to ensure a fresh split."
   ]
  },
  {
   "cell_type": "code",
   "id": "d2d5ec8796ff0e7d",
   "metadata": {},
   "source": [
    "plantvillage_root = Path(DATA_PROCESSED_DIR) / \"PlantVillage\"\n",
    "train_dir = plantvillage_root / \"train\"\n",
    "val_dir = plantvillage_root / \"val\"\n",
    "\n",
    "for d in (train_dir, val_dir):\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Train and validation directories are ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e93cbd47eccab135",
   "metadata": {},
   "source": [
    "## Step 5: Split the dataset\n",
    "We shuffle all images in each class and split them into:\n",
    "* 80% training data\n",
    "* 20% validation data\n",
    "\n",
    "Then, we copy (or move) the images into their respective class folders."
   ]
  },
  {
   "cell_type": "code",
   "id": "b6a0430f82e6231a",
   "metadata": {},
   "source": [
    "split_ratio = 0.8\n",
    "classes = [p for p in dataset_root.iterdir() if p.is_dir()]\n",
    "print(f\"Found {len(classes)} classes:\")\n",
    "\n",
    "image_exts = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "for class_path in classes:\n",
    "    print(\" -\", class_path.name)\n",
    "\n",
    "    images = [p for p in class_path.iterdir() if p.is_file() and p.suffix.lower() in image_exts]\n",
    "    if not images:\n",
    "        print(f\"No images found in {class_path.name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    random.shuffle(images)\n",
    "    split_idx = int(len(images) * split_ratio)\n",
    "    train_images = images[:split_idx]\n",
    "    val_images = images[split_idx:]\n",
    "\n",
    "    (train_dir / class_path.name).mkdir(parents=True, exist_ok=True)\n",
    "    (val_dir / class_path.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for img in train_images:\n",
    "        shutil.copy(img, train_dir / class_path.name / img.name)\n",
    "    for img in val_images:\n",
    "        shutil.copy(img, val_dir / class_path.name / img.name)\n",
    "\n",
    "    print(f\"{class_path.name}: {len(train_images)} train, {len(val_images)} val\")\n",
    "\n",
    "print(\"Dataset split completed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c693b5a05ac01f32",
   "metadata": {},
   "source": [
    "## Step 6: Verify the split\n",
    "We check that both the training and validation sets contain the expected number of images."
   ]
  },
  {
   "cell_type": "code",
   "id": "2deade7e0364e6b4",
   "metadata": {},
   "source": [
    "def count_images(folder):\n",
    "    exts = ('.jpg', '.jpeg', '.png')\n",
    "    return sum(1 for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts)\n",
    "\n",
    "train_count = count_images(train_dir)\n",
    "val_count = count_images(val_dir)\n",
    "\n",
    "print(f\"Total training images: {train_count}\")\n",
    "print(f\"Total validation images: {val_count}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3df89775f82632bd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We now have a clean and well-structured dataset ready for model training.\n",
    "\n",
    "* The dataset was extracted and split successfully.\n",
    "* Train and validation directories follow the same class hierarchy.\n",
    "* Randomization ensures a fair split for evaluation.\n",
    "\n",
    "Next step: we can load the datasets in TensorFlow or PyTorch using `image_dataset_from_directory()` and start training the CNN model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
